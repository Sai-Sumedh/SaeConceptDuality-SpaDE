/n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/../../functions/get_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  file = torch.load(datapath)
wandb: Currently logged in as: sumedh_hindupur (harvard01). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/wandb/run-20250214_041209-crw7sbpf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quick804_relu_gamreg0.001_021425
wandb: ⭐️ View project at https://wandb.ai/harvard01/intrinsic_dimX
wandb: 🚀 View run at https://wandb.ai/harvard01/intrinsic_dimX/runs/crw7sbpf
Traceback (most recent call last):
  File "/n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/trainmodel.py", line 305, in <module>
    update_status(f"Time to train {EPOCHS} epochs = {round(toc-tic,2)}s ({round((toc-tic)/EPOCHS,2)}s per epoch)")
NameError: name 'EPOCHS' is not defined
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb: 
wandb: Run history:
wandb: c0_loss_train_mse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c1_loss_train_mse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c2_loss_train_mse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c3_loss_train_mse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c4_loss_train_mse █▇▅▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            lambda ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        loss_train █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    loss_train_mse █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    loss_train_reg ▁▃▄▅▆▆▇▇████████▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆
wandb: 
wandb: Run summary:
wandb: c0_loss_train_mse 0.02104
wandb: c1_loss_train_mse 0.021
wandb: c2_loss_train_mse 0.03513
wandb: c3_loss_train_mse 0.0345
wandb: c4_loss_train_mse 0.05611
wandb:            lambda 0.00391
wandb:        loss_train 0.25678
wandb:    loss_train_mse 0.03307
wandb:    loss_train_reg 0.2237
wandb: 
wandb: 🚀 View run quick804_relu_gamreg0.001_021425 at: https://wandb.ai/harvard01/intrinsic_dimX/runs/crw7sbpf
wandb: ⭐️ View project at: https://wandb.ai/harvard01/intrinsic_dimX
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250214_041209-crw7sbpf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/../../functions/get_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  file = torch.load(datapath)
wandb: Currently logged in as: sumedh_hindupur (harvard01). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/wandb/run-20250214_145622-zqw8455w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run taxi170_relu_gamreg20.0_021425
wandb: ⭐️ View project at https://wandb.ai/harvard01/intrinsic_dimX
wandb: 🚀 View run at https://wandb.ai/harvard01/intrinsic_dimX/runs/zqw8455w
Traceback (most recent call last):
  File "/n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/trainmodel.py", line 305, in <module>
    update_status(f"Time to train {EPOCHS} epochs = {round(toc-tic,2)}s ({round((toc-tic)/EPOCHS,2)}s per epoch)")
NameError: name 'EPOCHS' is not defined
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb: 
wandb: Run history:
wandb: c0_loss_train_mse ▃▆▂▁▅▃▇▂▅▆▅▂▂▆▁▆▃▄▅▅▅▄▂▄█▇▄▅▇▅▃▄▆▄▄▄▄▃▂▆
wandb: c1_loss_train_mse ▅▆▄▂▂▅▃█▂▃▁▅▃▄▂▆▃▅▅▆▃▂▃▅▃▂▅▅▄▃▄▆▃▄▄▂▄▃▃▅
wandb: c2_loss_train_mse ▅▃▂▃▇▃▄▃▃▃▄▅▄▂▆▃▃▃▅█▂▅▄▃▂▃▆▃▅▁▃▆▅▃▆▁▆▄▃▅
wandb: c3_loss_train_mse █▄▁▃▂▅▂▂▃▄▅▃▃▃▅▅▃▄▄▄▅▁▄▅▄▄▆▃▃▅▃▃▄▃▃▆▃▅▄▄
wandb: c4_loss_train_mse █▆▄▅▃▄▂▅▁▃▄▄▁▄▃▂▂▃▂▃▃▂▁▂▄▃▃▂▃▃▃▁▃▄▃▂▃▂▄▁
wandb:            lambda ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        loss_train █▆▂▁▃▃▃▄▂▃▃▃▁▃▂▄▂▃▄▅▃▂▁▄▄▃▄▄▄▂▂▃▄▃▄▂▃▂▂▄
wandb:    loss_train_mse █▇▂▁▄▃▄▅▂▄▄▃▁▄▃▅▂▄▅▇▃▂▁▅▅▄▆▅▅▃▃▄▅▄▅▂▄▃▃▅
wandb:    loss_train_reg █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: c0_loss_train_mse 124.90108
wandb: c1_loss_train_mse 132.79625
wandb: c2_loss_train_mse 127.40434
wandb: c3_loss_train_mse 126.08373
wandb: c4_loss_train_mse 127.58434
wandb:            lambda 0.00391
wandb:        loss_train 127.69627
wandb:    loss_train_mse 127.69627
wandb:    loss_train_reg 0.0
wandb: 
wandb: 🚀 View run taxi170_relu_gamreg20.0_021425 at: https://wandb.ai/harvard01/intrinsic_dimX/runs/zqw8455w
wandb: ⭐️ View project at: https://wandb.ai/harvard01/intrinsic_dimX
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250214_145622-zqw8455w/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/../../functions/get_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  file = torch.load(datapath)
wandb: Currently logged in as: sumedh_hindupur (harvard01). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/wandb/run-20250214_151615-rwxp7rw3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crest18_relu_gamreg15.0_021425
wandb: ⭐️ View project at https://wandb.ai/harvard01/intrinsic_dimX
wandb: 🚀 View run at https://wandb.ai/harvard01/intrinsic_dimX/runs/rwxp7rw3
Traceback (most recent call last):
  File "/n/home00/sumedh/spmax_sae/experiments/expt16_intrindicdimX/trainmodel.py", line 305, in <module>
    update_status(f"Time to train {EPOCHS} epochs = {round(toc-tic,2)}s ({round((toc-tic)/EPOCHS,2)}s per epoch)")
NameError: name 'EPOCHS' is not defined
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb: 
wandb: Run history:
wandb: c0_loss_train_mse █▆▄▄▅▄▅▃▄▄▄▃▂▃▂▃▂▂▃▃▂▂▁▂▃▂▂▂▃▂▂▁▂▂▂▂▁▁▁▃
wandb: c1_loss_train_mse █▄▃▂▂▂▂▄▁▂▁▃▂▃▂▃▂▂▂▃▂▂▂▃▁▁▃▃▂▂▂▃▂▂▂▁▂▁▂▂
wandb: c2_loss_train_mse █▆▃▂▄▂▂▂▂▂▃▃▃▂▃▂▂▂▃▄▂▃▂▂▂▂▃▂▃▁▂▃▃▂▃▁▃▂▂▃
wandb: c3_loss_train_mse █▅▂▃▂▃▂▁▂▂▃▂▂▂▃▂▂▂▂▂▂▁▂▂▂▂▃▂▁▂▁▁▂▁▁▃▁▂▂▁
wandb: c4_loss_train_mse █▇▂▂▂▂▂▃▁▂▂▂▁▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▂▁
wandb:            lambda ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        loss_train █▆▃▂▃▃▃▄▂▃▃▃▁▃▂▃▁▃▄▅▂▁▁▃▃▂▄▃▃▂▂▃▃▂▃▁▂▂▂▃
wandb:    loss_train_mse █▅▃▂▃▂▂▃▂▂▂▂▁▂▂▂▁▂▂▂▂▁▁▂▂▁▂▂▂▁▁▁▂▁▂▁▁▁▁▂
wandb:    loss_train_reg ▁▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██████████████████
wandb: 
wandb: Run summary:
wandb: c0_loss_train_mse 108.78975
wandb: c1_loss_train_mse 120.04891
wandb: c2_loss_train_mse 119.73792
wandb: c3_loss_train_mse 119.38929
wandb: c4_loss_train_mse 120.11578
wandb:            lambda 0.00391
wandb:        loss_train 126.8115
wandb:    loss_train_mse 117.60495
wandb:    loss_train_reg 9.20655
wandb: 
wandb: 🚀 View run crest18_relu_gamreg15.0_021425 at: https://wandb.ai/harvard01/intrinsic_dimX/runs/rwxp7rw3
wandb: ⭐️ View project at: https://wandb.ai/harvard01/intrinsic_dimX
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250214_151615-rwxp7rw3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/n/home00/sumedh/spmax_sae/experiments/expt16_intrinsicdimX/../../functions/get_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  file = torch.load(datapath)
wandb: Currently logged in as: sumedh_hindupur (harvard01). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /n/home00/sumedh/spmax_sae/experiments/expt16_intrinsicdimX/wandb/run-20250218_143706-db2n8aht
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run math932_jumprelu_gamreg0.025_021825
wandb: ⭐️ View project at https://wandb.ai/harvard01/intrinsic_dimX
wandb: 🚀 View run at https://wandb.ai/harvard01/intrinsic_dimX/runs/db2n8aht
Traceback (most recent call last):
  File "/n/home00/sumedh/spmax_sae/experiments/expt16_intrinsicdimX/trainmodel.py", line 305, in <module>
    update_status(f"Time to train {EPOCHS} epochs = {round(toc-tic,2)}s ({round((toc-tic)/EPOCHS,2)}s per epoch)")
NameError: name 'EPOCHS' is not defined
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.020 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb: 
wandb: Run history:
wandb: c0_loss_train_mse █▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c1_loss_train_mse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c2_loss_train_mse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c3_loss_train_mse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: c4_loss_train_mse █▇▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            lambda ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        loss_train █▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    loss_train_mse █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    loss_train_reg ███▇▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: c0_loss_train_mse 0.23497
wandb: c1_loss_train_mse 0.22695
wandb: c2_loss_train_mse 0.29863
wandb: c3_loss_train_mse 0.33169
wandb: c4_loss_train_mse 0.55067
wandb:            lambda 0.00391
wandb:        loss_train 5.97252
wandb:    loss_train_mse 0.32342
wandb:    loss_train_reg 5.6491
wandb: 
wandb: 🚀 View run math932_jumprelu_gamreg0.025_021825 at: https://wandb.ai/harvard01/intrinsic_dimX/runs/db2n8aht
wandb: ⭐️ View project at: https://wandb.ai/harvard01/intrinsic_dimX
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_143706-db2n8aht/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
