/n/home00/sumedh/spmax_sae/experiments/expt19_separability_diffmag2/../../functions/get_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  file = torch.load(datapath)
wandb: Currently logged in as: sumedh_hindupur (harvard01). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /n/home00/sumedh/spmax_sae/experiments/expt19_separability_diffmag2/wandb/run-20250217_144808-ovwb6507
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run nasty521_relu_gamreg5.0_021725
wandb: â­ï¸ View project at https://wandb.ai/harvard01/separability_2dgauss_diffmag
wandb: ğŸš€ View run at https://wandb.ai/harvard01/separability_2dgauss_diffmag/runs/ovwb6507
wandb: - 0.008 MB of 0.019 MB uploadedwandb: \ 0.008 MB of 0.019 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: c0_loss_train_mse â–â–†â–†â–ˆâ–„â–„â–†â–‡â–†â–†â–…â–†â–„â–‡â–…â–†â–„â–†â–…â–†â–…â–‡â–†â–…â–…â–ˆâ–„â–…â–†â–…â–†â–†â–†â–†â–†â–‡â–†â–‡â–…â–†
wandb: c1_loss_train_mse â–â–ˆâ–‡â–‡â–†â–ˆâ–‡â–‡â–†â–‡â–‡â–…â–‡â–‡â–†â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–…â–‡â–‡â–ˆâ–ˆâ–†â–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: c2_loss_train_mse â–â–‡â–‡â–‡â–†â–‡â–‡â–†â–…â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–†â–†â–†â–‡â–†â–†â–‡â–†â–‡â–†â–ˆâ–‡â–†â–†â–‡â–‡â–†â–‡â–ˆ
wandb: c3_loss_train_mse â–ˆâ–‚â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–â–â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒ
wandb: c4_loss_train_mse â–ˆâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–‚â–ƒâ–ƒâ–„â–„â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚
wandb: c5_loss_train_mse â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–‚â–„â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒ
wandb:            lambda â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        loss_train â–ˆâ–„â–‡â–…â–â–ƒâ–…â–†â–†â–‚â–…â–„â–ƒâ–ƒâ–„â–…â–„â–…â–„â–†â–‚â–…â–‚â–‚â–‡â–„â–„â–„â–„â–†â–…â–†â–„â–‡â–†â–‚â–‡â–„â–„â–ƒ
wandb:    loss_train_mse â–…â–…â–‡â–…â–â–ƒâ–†â–‡â–‡â–ƒâ–…â–„â–ƒâ–ƒâ–„â–†â–„â–†â–„â–‡â–‚â–†â–‚â–ƒâ–ˆâ–„â–…â–„â–…â–†â–†â–‡â–…â–ˆâ–†â–ƒâ–ˆâ–„â–…â–ƒ
wandb:    loss_train_reg â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: c0_loss_train_mse 3.53616
wandb: c1_loss_train_mse 0.30459
wandb: c2_loss_train_mse 3.61873
wandb: c3_loss_train_mse 0.43448
wandb: c4_loss_train_mse 3.46392
wandb: c5_loss_train_mse 0.50906
wandb:            lambda 0.25
wandb:        loss_train 2.1095
wandb:    loss_train_mse 2.1095
wandb:    loss_train_reg 0.0
wandb: 
wandb: ğŸš€ View run nasty521_relu_gamreg5.0_021725 at: https://wandb.ai/harvard01/separability_2dgauss_diffmag/runs/ovwb6507
wandb: â­ï¸ View project at: https://wandb.ai/harvard01/separability_2dgauss_diffmag
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250217_144808-ovwb6507/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/n/home00/sumedh/spmax_sae/experiments/expt19_separability_diffmag2/../../functions/get_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  file = torch.load(datapath)
wandb: Currently logged in as: sumedh_hindupur (harvard01). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /n/home00/sumedh/spmax_sae/experiments/expt19_separability_diffmag2/wandb/run-20250217_161237-bwwpjzwe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fun523_relu_gamreg0.05_w128_021725
wandb: â­ï¸ View project at https://wandb.ai/harvard01/separability_2dgauss_diffmag
wandb: ğŸš€ View run at https://wandb.ai/harvard01/separability_2dgauss_diffmag/runs/bwwpjzwe
wandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: c0_loss_train_mse â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c1_loss_train_mse â–ˆâ–‡â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c2_loss_train_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c3_loss_train_mse â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c4_loss_train_mse â–ˆâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c5_loss_train_mse â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            lambda â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        loss_train â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    loss_train_mse â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    loss_train_reg â–ˆâ–…â–ƒâ–‚â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–‚â–‚â–â–‚â–â–â–
wandb: 
wandb: Run summary:
wandb: c0_loss_train_mse 0.00058
wandb: c1_loss_train_mse 0.00213
wandb: c2_loss_train_mse 0.00057
wandb: c3_loss_train_mse 0.00302
wandb: c4_loss_train_mse 0.00067
wandb: c5_loss_train_mse 0.003
wandb:            lambda 0.25
wandb:        loss_train 0.07692
wandb:    loss_train_mse 0.00152
wandb:    loss_train_reg 0.0754
wandb: 
wandb: ğŸš€ View run fun523_relu_gamreg0.05_w128_021725 at: https://wandb.ai/harvard01/separability_2dgauss_diffmag/runs/bwwpjzwe
wandb: â­ï¸ View project at: https://wandb.ai/harvard01/separability_2dgauss_diffmag
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250217_161237-bwwpjzwe/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/n/home00/sumedh/spmax_sae/experiments/expt19_separability_diffmag2/../../functions/get_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  file = torch.load(datapath)
wandb: Currently logged in as: sumedh_hindupur (harvard01). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /n/home00/sumedh/spmax_sae/experiments/expt19_separability_diffmag2/wandb/run-20250217_194535-mdhi00u0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radio455_topk_relu_k2_w128_021725
wandb: â­ï¸ View project at https://wandb.ai/harvard01/separability_2dgauss_diffmag
wandb: ğŸš€ View run at https://wandb.ai/harvard01/separability_2dgauss_diffmag/runs/mdhi00u0
wandb: - 0.008 MB of 0.022 MB uploadedwandb: \ 0.008 MB of 0.022 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: c0_loss_train_mse â–ˆâ–ƒâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c1_loss_train_mse â–†â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c2_loss_train_mse â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c3_loss_train_mse â–ˆâ–„â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c4_loss_train_mse â–ˆâ–ƒâ–‚â–„â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: c5_loss_train_mse â–ˆâ–‡â–„â–„â–„â–…â–…â–…â–…â–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            lambda â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        loss_train â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    loss_train_mse â–ˆâ–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    loss_train_reg â–ˆâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: c0_loss_train_mse 0.0
wandb: c1_loss_train_mse 0.0
wandb: c2_loss_train_mse 0.0
wandb: c3_loss_train_mse 0.0
wandb: c4_loss_train_mse 0.0
wandb: c5_loss_train_mse 0.0
wandb:            lambda 0.25
wandb:        loss_train 0.0
wandb:    loss_train_mse 0.0
wandb:    loss_train_reg 0.0
wandb: 
wandb: ğŸš€ View run radio455_topk_relu_k2_w128_021725 at: https://wandb.ai/harvard01/separability_2dgauss_diffmag/runs/mdhi00u0
wandb: â­ï¸ View project at: https://wandb.ai/harvard01/separability_2dgauss_diffmag
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250217_194535-mdhi00u0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
